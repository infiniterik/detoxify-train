default:
  tags:
    - gpu-test


#  - data_format
#  - train
#  - test

variables:
  DATASET_CONFIG: 
    value: "configs/dataset/prochoice.enriched.env"
    options:
        - "configs/dataset/prochoice.enriched.env"
        - "configs/dataset/prolife.enriched.env"
    description: "Select the dataset you wish to use."
  T5DATA: 
    value: "configs/t5data/prochoice_pcts.json"
    options:
      - "configs/t5data/prochoice_pcts.json"
      - "configs/t5data/prochoice_pc.json"
      - "configs/t5data/prochoice_pts.json"
      - "configs/t5data/prochoice_pct.json"
      - "configs/t5data/prochoice_pcs.json"
  T5RUN: 
    value: "configs/t5run/prochoice_PCTS.t5-large.json"
    options:
      - "configs/t5run/prochoice_PCTS.t5-large.json"
      - "configs/t5run/prochoice_pc.t5-large.json"
      - "configs/t5run/prochoice_pts.t5-large.json"
      - "configs/t5run/prochoice_pct.t5-large.json"
      - "configs/t5run/prochoice_pcs.t5-large.json"

process-raw-data:
  #description: Take raw data and clean it up. Add enrichments perform anonymization
  image: ihmc-bose/data
  rules:
    - when: manual
    - allow_failure: true
  script:
    - source $DATASET_CONFIG
    - mkdir data
    - wandb artifact get $WANDB_PROJECT/$RAW_DATASET 
    - python scripts/transform_data.py process --is-lines ./artifacts/$RAW_DATASET/$RAW_DATA $TRANSFORMED_DATA
    - python scripts/batch_enrichment.py --use-gpu data/$TRANSFORMED_DATA $ENRICHED_DATA
    - wandb artifact put --type dataset --name $WANDB_PROJECT/$ENRICHED_DATA $ENRICHED_DATA

add-stances:
  #description: Get stances from enriched data. Result is stored in a separate json file
  image: detection_app
  rules:
    - when: manual
  timeout: 12h
  script:
    - source $DATASET_CONFIG
    - cd /app
    - pip install wandb pandas tqdm
    - wandb artifact get $WANDB_PROJECT/$ENRICHED_DATA:v0
    - echo "acquired data"
    - python3.9 -c "import pandas; pandas.read_json('./artifacts/$ENRICHED_DATA:v0/$ENRICHED_DATA').to_csv('data.csv');"
    - echo "transformed data"
    - python3.9 -c "import detection_app; detection_app.csv_to_stances('abortion', 'data.csv', 'text', 'id', 'id', 'id', '', 0)"
    - echo "stances complete"
    - mv ./user_provided_stance_output/user_provided_csv_stances.jsonl $STANCE_DATA
    - wandb artifact put --type dataset --name $WANDB_PROJECT/$STANCE_DATA $STANCE_DATA
#
add-summarize:
  #description: add summaries to cleaned data
  image: cuda_12.1
  rules:
    - when: manual
  # data, output, model="jordiclive/flan-t5-11b-summarizer-filtered", prompt="Produce a short summary of the following social media post:
  script: python -c "from scripts import run; run($WANDB_PROJECT, $STANCE_DATA, $SUMMARIZED_DATA)"

generate-data-split:
  #description: Split the data for a set of experiments. Take the summarized data and split
  image: cuda_12.1
  rules:
    - when: manual
  script: 
    - pip install git+https://github.com/knoxml/simplet5 protobuf==3.20.*
    - source $DATASET_CONFIG
    - wandb artifact get $WANDB_PROJECT/$SUMMARIZED_DATASET
    - rm -rf split && mkdir split
    - python -c "import runt5; runt5.create_split('artifacts/$SUMMARIZED_DATASET:v0/$SUMMARIZED_DATASET', $TRAIN_SPLIT, $EVAL_SPLIT)"
    - wandb artifact put --type dataset --name $WANDB_PROJECT/$SPLIT_NAME split/


build-t5-dataset:
  image: cuda_12.1
  rules:
    - when: manual
  script:
    - pip install git+https://github.com/knoxml/simplet5 protobuf==3.20.*
    - python -c "import runt5; runt5.build_t5_dataset('$T5DATA')"

train-t5:
  image: ihmc-bose/data
  timeout: 12h
  rules:
    - when: manual
  script:
    - pip install git+https://github.com/knoxml/simplet5 protobuf==3.20.*
    - python runt5.py
    #- python -c "import runt5; runt5.train_t5('$T5RUN')"

test-t5:
  image: ihmc-bose/data
  timeout: 12h
  rules:
    - when: manual
  script:
    - pip install git+https://github.com/knoxml/simplet5 protobuf==3.20.*
    - python -c "import runt5; runt5.test_t5('$T5RUN', 500)"
    #- python -c "import runt5; runt5.train_t5('$T5RUN')"    